from langchain_community.llms import Ollama
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
import warnings
from rich.console import Console
from rich.panel import Panel
from rich.text import Text

warnings.filterwarnings("ignore")

# 1. æ–‡æ¡£å¤„ç†æµæ°´çº¿
def process_documents():
    loader = WebBaseLoader("https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/")
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=14,
        separators=["\n\n", "\n", " ", ".", ",", "", ";", "!", "?"],
        length_function=len,
    )
    return text_splitter.split_documents(documents)

# 2. æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–
def init_models():
    # åˆå§‹åŒ– Embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # åˆå§‹åŒ–æœ¬åœ°å¤§æ¨¡å‹
    llm = Ollama(
        model="qwen2.5-coder:14b",
        temperature=0.5,
    )
    
    return embeddings, llm

# 3. RAG pipeline æ„å»º
def build_rag_pipeline(docs, embeddings, llm):
    # åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = FAISS.from_documents(docs, embeddings)
    
    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vector_store.as_retriever(search_kwargs={"k": 4})
    
    # å®šä¹‰æç¤ºæ¨¡æ¿
    prompt_template = ChatPromptTemplate.from_template("""ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”é—®é¢˜ï¼š
    {context}
    
    é—®é¢˜ï¼š{question}
    ç­”æ¡ˆï¼š""")
    
    # å®šä¹‰æ ¼å¼åŒ–ä¸Šä¸‹æ–‡çš„å‡½æ•°
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    # æ„å»ºLCEL RAGé“¾
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | StrOutputParser()
    )
    
    # åŒ…è£…é“¾ä»¥è¿”å›æºæ–‡æ¡£
    def invoke_with_sources(query):
        docs = retriever.invoke(query)
        result = rag_chain.invoke(query)
        return {"result": result, "source_documents": docs}
    
    return invoke_with_sources

if __name__ == "__main__":
    
    # åˆå§‹åŒ–ç»„ä»¶
    split_docs = process_documents()
    embedding_model, llm_model = init_models()
    
    # æ„å»ºRAGç³»ç»Ÿ
    qa_chain = build_rag_pipeline(split_docs, embedding_model, llm_model)
    
    # åˆ›å»ºRichæ§åˆ¶å°
    console = Console()
    
    # äº¤äº’é—®ç­”
    while True:
        query = input("\nè¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥qé€€å‡ºï¼‰: ")
        if query.lower() == 'q':
            break
            
        result = qa_chain(query)
        
        # ä½¿ç”¨Richç¾åŒ–è¾“å‡º
        answer_text = Text(result['result'])
        console.print(Panel(answer_text, title="ğŸ“ å›ç­”", border_style="green"))
        
        # å¤„ç†æ¥æºæ–‡æ¡£
        sources_content = ""
        for i, doc in enumerate(result['source_documents']):
            doc_content = doc.page_content[:150] + "..." if len(doc.page_content) > 150 else doc.page_content
            sources_content += f"\n[bold]æ¥æº {i+1}:[/bold]\n"
            sources_content += f"  - å†…å®¹ç‰‡æ®µ: {doc_content}\n"
            if hasattr(doc, 'metadata') and doc.metadata:
                sources_content += f"  - å…ƒæ•°æ®: {doc.metadata}\n"
        
        console.print(Panel(sources_content, title="ğŸ“š å‚è€ƒæ¥æº", border_style="blue"))
